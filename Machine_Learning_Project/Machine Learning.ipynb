{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify predictions as boolean values\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Preprocess values and remove future predictions column\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future', 1)\n",
    "    \n",
    "    # Drop non-numeric rows and change prices to percentage change values, then scale each column\n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Sort values into sequential chunks before randomizing for more accurate model predictions\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "            \n",
    "    # Randomize the sequence\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    # If future price is greater than current price, buy, if future price less than current price, sell\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    # Randomize each buy or sell\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    # Balance buys/sells by creating equal length lists for buys/sells\n",
    "    # Use the lowest length value of the two lists as constraints\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 600 validation: 52\n",
      "Dont buys: 300, buys: 300\n",
      "TEST Dont buys: 26, buys: 26\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 600 samples, validate on 52 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "600/600 [==============================] - 5s 8ms/sample - loss: 0.9845 - acc: 0.5067 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.8269 - acc: 0.5317 - val_loss: 0.6957 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.8073 - acc: 0.5317 - val_loss: 0.6962 - val_acc: 0.5385\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 1s 938us/sample - loss: 0.7534 - acc: 0.5583 - val_loss: 0.6959 - val_acc: 0.5385\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.7316 - acc: 0.5967 - val_loss: 0.6965 - val_acc: 0.4423\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 1s 1ms/sample - loss: 0.7361 - acc: 0.5667 - val_loss: 0.6965 - val_acc: 0.3654\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 1s 925us/sample - loss: 0.7907 - acc: 0.4933 - val_loss: 0.6969 - val_acc: 0.3846\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.7469 - acc: 0.5400 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.7139 - acc: 0.5500 - val_loss: 0.6967 - val_acc: 0.4423\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 1s 2ms/sample - loss: 0.7117 - acc: 0.5850 - val_loss: 0.7002 - val_acc: 0.4423\n",
      "----------------\n",
      "BTC:\n",
      "1=buy 0=sell actual values:\n",
      "[0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1.]\n",
      "predicted values:\n",
      "[0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "----------------\n",
      "train data: 620 validation: 50\n",
      "Dont buys: 310, buys: 310\n",
      "TEST Dont buys: 25, buys: 25\n",
      "Train on 620 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "620/620 [==============================] - 6s 10ms/sample - loss: 0.9426 - acc: 0.5242 - val_loss: 0.6961 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.8285 - acc: 0.5597 - val_loss: 0.6987 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 2s 3ms/sample - loss: 0.8121 - acc: 0.5500 - val_loss: 0.6968 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.7341 - acc: 0.5661 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.7250 - acc: 0.5677 - val_loss: 0.6958 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 1s 994us/sample - loss: 0.7094 - acc: 0.5694 - val_loss: 0.6952 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.7133 - acc: 0.6065 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.6786 - acc: 0.6081 - val_loss: 0.6941 - val_acc: 0.5200\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.6962 - acc: 0.5790 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 2s 3ms/sample - loss: 0.6788 - acc: 0.6032 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "----------------\n",
      "LTC:\n",
      "1=buy 0=sell actual values:\n",
      "[0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0.]\n",
      "predicted values:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "----------------\n",
      "train data: 636 validation: 54\n",
      "Dont buys: 318, buys: 318\n",
      "TEST Dont buys: 27, buys: 27\n",
      "Train on 636 samples, validate on 54 samples\n",
      "Epoch 1/10\n",
      "636/636 [==============================] - 8s 13ms/sample - loss: 0.8219 - acc: 0.5314 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "636/636 [==============================] - 1s 1ms/sample - loss: 0.7982 - acc: 0.5252 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "636/636 [==============================] - 1s 1ms/sample - loss: 0.7464 - acc: 0.5440 - val_loss: 0.6925 - val_acc: 0.4630\n",
      "Epoch 4/10\n",
      "636/636 [==============================] - 1s 1ms/sample - loss: 0.6884 - acc: 0.5943 - val_loss: 0.6917 - val_acc: 0.5370\n",
      "Epoch 5/10\n",
      "636/636 [==============================] - 1s 1ms/sample - loss: 0.6815 - acc: 0.6022 - val_loss: 0.6915 - val_acc: 0.6111\n",
      "Epoch 6/10\n",
      "636/636 [==============================] - 1s 2ms/sample - loss: 0.7043 - acc: 0.5881 - val_loss: 0.6908 - val_acc: 0.6481\n",
      "Epoch 7/10\n",
      "636/636 [==============================] - 1s 2ms/sample - loss: 0.6818 - acc: 0.6116 - val_loss: 0.6900 - val_acc: 0.6111\n",
      "Epoch 8/10\n",
      "636/636 [==============================] - 1s 1ms/sample - loss: 0.6449 - acc: 0.6321 - val_loss: 0.6887 - val_acc: 0.5741\n",
      "Epoch 9/10\n",
      "636/636 [==============================] - 1s 2ms/sample - loss: 0.6560 - acc: 0.6242 - val_loss: 0.6903 - val_acc: 0.5185\n",
      "Epoch 10/10\n",
      "636/636 [==============================] - 1s 2ms/sample - loss: 0.6406 - acc: 0.6415 - val_loss: 0.6864 - val_acc: 0.6296\n",
      "----------------\n",
      "ETH:\n",
      "1=buy 0=sell actual values:\n",
      "[0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 0.]\n",
      "predicted values:\n",
      "[0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0]\n",
      "----------------\n",
      "train data: 354 validation: 28\n",
      "Dont buys: 177, buys: 177\n",
      "TEST Dont buys: 14, buys: 14\n",
      "Train on 354 samples, validate on 28 samples\n",
      "Epoch 1/10\n",
      "354/354 [==============================] - 9s 25ms/sample - loss: 1.0705 - acc: 0.4831 - val_loss: 0.6929 - val_acc: 0.4643\n",
      "Epoch 2/10\n",
      "354/354 [==============================] - 1s 3ms/sample - loss: 0.8697 - acc: 0.5847 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "354/354 [==============================] - 1s 4ms/sample - loss: 0.8347 - acc: 0.5508 - val_loss: 0.6940 - val_acc: 0.4286\n",
      "Epoch 4/10\n",
      "354/354 [==============================] - 1s 3ms/sample - loss: 0.7592 - acc: 0.5847 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "354/354 [==============================] - 1s 3ms/sample - loss: 0.6558 - acc: 0.6215 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "354/354 [==============================] - 1s 3ms/sample - loss: 0.6824 - acc: 0.6412 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "354/354 [==============================] - 2s 4ms/sample - loss: 0.7076 - acc: 0.6073 - val_loss: 0.6927 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "354/354 [==============================] - 1s 2ms/sample - loss: 0.6233 - acc: 0.6497 - val_loss: 0.6933 - val_acc: 0.4643\n",
      "Epoch 9/10\n",
      "354/354 [==============================] - 1s 4ms/sample - loss: 0.6731 - acc: 0.6017 - val_loss: 0.6956 - val_acc: 0.4643\n",
      "Epoch 10/10\n",
      "354/354 [==============================] - 1s 2ms/sample - loss: 0.6381 - acc: 0.6554 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "----------------\n",
      "AMZN:\n",
      "1=buy 0=sell actual values:\n",
      "[0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 1.]\n",
      "predicted values:\n",
      "[0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "----------------\n",
      "train data: 370 validation: 30\n",
      "Dont buys: 185, buys: 185\n",
      "TEST Dont buys: 15, buys: 15\n",
      "Train on 370 samples, validate on 30 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "370/370 [==============================] - 11s 30ms/sample - loss: 1.0563 - acc: 0.5459 - val_loss: 0.6909 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "370/370 [==============================] - 1s 4ms/sample - loss: 0.8639 - acc: 0.5784 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "370/370 [==============================] - 1s 3ms/sample - loss: 0.7882 - acc: 0.5838 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "370/370 [==============================] - 2s 5ms/sample - loss: 0.8351 - acc: 0.5541 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "370/370 [==============================] - 1s 3ms/sample - loss: 0.8323 - acc: 0.5568 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "370/370 [==============================] - 2s 5ms/sample - loss: 0.7832 - acc: 0.5865 - val_loss: 0.6920 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "370/370 [==============================] - 1s 3ms/sample - loss: 0.7116 - acc: 0.6081 - val_loss: 0.6905 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "370/370 [==============================] - 2s 5ms/sample - loss: 0.6896 - acc: 0.6459 - val_loss: 0.6891 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "370/370 [==============================] - 1s 3ms/sample - loss: 0.6259 - acc: 0.6459 - val_loss: 0.6884 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "370/370 [==============================] - 1s 3ms/sample - loss: 0.6352 - acc: 0.6459 - val_loss: 0.6896 - val_acc: 0.5000\n",
      "----------------\n",
      "GOOGL:\n",
      "1=buy 0=sell actual values:\n",
      "[0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1.]\n",
      "predicted values:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "----------------\n",
      "train data: 402 validation: 24\n",
      "Dont buys: 201, buys: 201\n",
      "TEST Dont buys: 12, buys: 12\n",
      "Train on 402 samples, validate on 24 samples\n",
      "Epoch 1/10\n",
      "402/402 [==============================] - 13s 32ms/sample - loss: 0.9149 - acc: 0.5448 - val_loss: 0.6958 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.8537 - acc: 0.5249 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.7733 - acc: 0.5672 - val_loss: 0.6917 - val_acc: 0.5417\n",
      "Epoch 4/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.7808 - acc: 0.5672 - val_loss: 0.6901 - val_acc: 0.5417\n",
      "Epoch 5/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.7614 - acc: 0.5871 - val_loss: 0.6859 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.7768 - acc: 0.5473 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 7/10\n",
      "402/402 [==============================] - 2s 4ms/sample - loss: 0.7396 - acc: 0.5672 - val_loss: 0.6872 - val_acc: 0.5833\n",
      "Epoch 8/10\n",
      "402/402 [==============================] - 2s 4ms/sample - loss: 0.6956 - acc: 0.5945 - val_loss: 0.6847 - val_acc: 0.5417\n",
      "Epoch 9/10\n",
      "402/402 [==============================] - 1s 2ms/sample - loss: 0.7121 - acc: 0.5846 - val_loss: 0.6791 - val_acc: 0.5833\n",
      "Epoch 10/10\n",
      "402/402 [==============================] - 1s 3ms/sample - loss: 0.7230 - acc: 0.5672 - val_loss: 0.6830 - val_acc: 0.5000\n",
      "----------------\n",
      "FB:\n",
      "1=buy 0=sell actual values:\n",
      "[1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1.]\n",
      "predicted values:\n",
      "[0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 7                   # 1 week of data\n",
    "FUTURE_PERIOD_PREDICT = 2     # 2 days in the future\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "main_df = pd.DataFrame()\n",
    "assets = [\"BTC\", \"LTC\", \"ETH\", \"AMZN\", \"GOOGL\", \"FB\"]\n",
    "\n",
    "# Main loop to plug each asset into the machine learning model\n",
    "for asset in assets:\n",
    "    NAME = f\"{asset}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "    \n",
    "    # Sub-loop all data to build dataframe; add csv columns into main_df\n",
    "    Assets = [\"BTC\", \"LTC\", \"ETH\", \"AMZN\", \"GOOGL\", \"FB\"]\n",
    "    for Asset in Assets:\n",
    "        dataset = f\"Data/{asset}.csv\"\n",
    "    \n",
    "        df = pd.read_csv(dataset)\n",
    "        df.rename(columns={\"close\": f\"{Asset}_close\", \"volume\": f\"{Asset}_volume\"}, inplace=True)\n",
    "    \n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df[[f\"{Asset}_close\", f\"{Asset}_volume\"]]\n",
    "    \n",
    "        if len(main_df) == 0:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "        \n",
    "\n",
    "    main_df['future'] = main_df[f\"{asset}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "    main_df['target'] = list(map(classify, main_df[f\"{asset}_close\"], main_df[\"future\"]))\n",
    "    # print(main_df[[f\"{COIN_TO_PREDICT}_close\",\"future\",\"target\"]].head(10))\n",
    "\n",
    "\n",
    "    # Sort dataframes by date and set last 10% of dates as a variable\n",
    "    times = sorted(main_df.index.values)\n",
    "    last_10pct = times[-int(0.1*len(times))]\n",
    "\n",
    "\n",
    "    # Split first 90% of data for training data and last 10% as test data\n",
    "    validate_main_df = main_df[(main_df.index >= last_10pct)]\n",
    "    main_df = main_df[(main_df.index < last_10pct)]\n",
    "\n",
    "    train_x, train_y = preprocess_df(main_df)\n",
    "    validate_x, validate_y = preprocess_df(validate_main_df)\n",
    "\n",
    "    print(f\"train data: {len(train_x)} validation: {len(validate_x)}\")\n",
    "    print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "    print(f\"TEST Dont buys: {validate_y.count(0)}, buys: {validate_y.count(1)}\")\n",
    "\n",
    "    # Initialize sequential model and add a few layers for better accuracy\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Final layer has 2 output nodes for binary classification\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=opt,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Visualize model training\n",
    "    tensorboard = TensorBoard(log_dir= f'logs/{NAME}')\n",
    "\n",
    "    filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "    checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "    history = model.fit(train_x, train_y, batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS, validation_data=(validate_x, validate_y),\n",
    "                       callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "    # Output predictions: 1=buy, 0=sell     \n",
    "    predictions = np.argmax(model.predict(validate_x), axis=1)\n",
    "\n",
    "    # Actual test values to compare against predictions\n",
    "    actual = np.array(validate_y).ravel()\n",
    "    \n",
    "    print(\"----------------\")\n",
    "    print(f'{asset}:')\n",
    "    print(\"1=buy 0=sell actual values:\")\n",
    "    print(actual)\n",
    "    print(\"predicted values:\")\n",
    "    print(predictions)\n",
    "    print(\"----------------\")\n",
    "    \n",
    "#     predictions = pd.DataFrame({f'{asset}': pred_x})\n",
    "    \n",
    "#     if len(main_df) == 0:\n",
    "#         data_df = ({'actual values': actual})\n",
    "#         data_df.join(predictions)\n",
    "#     else:\n",
    "#         data_df = data_df.join(predictions)\n",
    "    \n",
    "\n",
    "    # Clears dataframes before next iteration through loop\n",
    "#     predictions = predictions.iloc[0:0]\n",
    "    main_df = main_df.iloc[0:0]\n",
    "    df = df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
